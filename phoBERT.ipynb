{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset"
      ],
      "metadata": {
        "id": "WA06MXiTPxye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielphamvt/sentiment_analysis_nal.git"
      ],
      "metadata": {
        "id": "_pz2wUp_P0if",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315554f2-47b0-4694-e85a-4c20c60096a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sentiment_analysis_nal'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Receiving objects: 100% (33/33), 1.19 MiB | 6.78 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataSA"
      ],
      "metadata": {
        "id": "1s8MaF9GQFYG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/sentiment_analysis_nal/data_clean/train.crash /content/dataSA"
      ],
      "metadata": {
        "id": "jUXRxudlQSJl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/sentiment_analysis_nal/data_clean/test.crash /content/dataSA"
      ],
      "metadata": {
        "id": "Mp2-sxCnQbwz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r sentiment_analysis_nal"
      ],
      "metadata": {
        "id": "VLh4Xj-TQfjR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare transformer and library"
      ],
      "metadata": {
        "id": "JuRZB1naPrwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-buyo7gvpQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7339d3b-f014-498b-a509-25736437c426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 221326, done.\u001b[K\n",
            "remote: Counting objects: 100% (570/570), done.\u001b[K\n",
            "remote: Compressing objects: 100% (327/327), done.\u001b[K\n",
            "remote: Total 221326 (delta 308), reused 361 (delta 181), pack-reused 220756\u001b[K\n",
            "Receiving objects: 100% (221326/221326), 242.32 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (160364/160364), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd transformers\n"
      ],
      "metadata": {
        "id": "QtXYSLiLv6J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastBPE"
      ],
      "metadata": {
        "id": "rS_nadJ1wJi_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a856fed6-b528-49c8-ea72-5d55f82c568f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fastBPE\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp310-cp310-linux_x86_64.whl size=806584 sha256=0926934ba7cb7d059c40f020a613163d47b26cbbe05823e3b5e6e27c08640c43\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/5d/b9/4b8897941ebc9e8c6cc3f3ffd3ea5115731754269205098754\n",
            "Successfully built fastBPE\n",
            "Installing collected packages: fastBPE\n",
            "Successfully installed fastBPE-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cant install fairseq with pip 24.2\n",
        "!pip install pip==24.0"
      ],
      "metadata": {
        "id": "uBivOrmU5HpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1892e8-3214-443c-8da8-43dc7c006ce7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip==24.0\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq"
      ],
      "metadata": {
        "id": "1lMbTeAh3gfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95275652-fd05-4a2a-ea5f-0f9c61d3f049"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq)\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.5.15)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.4)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Using cached bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.26.4)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->fairseq)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m917.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11289133 sha256=c0fa343cfb29e725c5230367168da49936492b026dafc49941636c57f3fd49a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=a7e225c56d3d7d7dd8ed95d1808475e09062ae1116a79cfa4181a5b9e6bf1a28\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.10.1 sacrebleu-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fastBPE is a tool for efficient tokenization using the BPE algorithm, while fairseq is a versatile toolkit for training and deploying state-of-the-art NLP models"
      ],
      "metadata": {
        "id": "C0zfddNB9uIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_gMbjTLxsxG",
        "outputId": "d2ce01f6-26be-47db-8b32-fd10d0fed7e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the vncorenlp python wrapper\n",
        "!pip install vncorenlp\n",
        "\n",
        "# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "id": "srH-mjh7yI62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff727bb-d611-4032-a2ee-af78c5c27591"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2024.7.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=36c9912e1a9b201450205f98bf1abb5f38c8dabea384a856c9a9489926b57caa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "--2024-07-30 05:47:51--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-30 05:47:52 (177 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2024-07-30 05:47:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-07-30 05:47:52 (11.0 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2024-07-30 05:47:52--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-30 05:47:52 (4.92 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "\n",
        "text = \"Tôi là học sinh của trường Khoa học Tự Nhiên TPHCM. Tôi thích mèo.\"\n",
        "\n",
        "word_segmented_text = rdrsegmenter.tokenize(text)\n",
        "print(word_segmented_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ4mAC8RyaAY",
        "outputId": "793ae305-8fba-4bdd-c30b-68376906cb07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Tôi', 'là', 'học_sinh', 'của', 'trường', 'Khoa_học_Tự_Nhiên', 'TPHCM', '.'], ['Tôi', 'thích', 'mèo', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rdrsegmenter is basically the word tokenizer for vnmese"
      ],
      "metadata": {
        "id": "gJ2fApol0Pc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the tokenizer can split between sentence"
      ],
      "metadata": {
        "id": "_KluIErK0bpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the pretrain for phoBERT\n",
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz\n"
      ],
      "metadata": {
        "id": "WLB-CEA70UrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a8612c-5f05-4df2-edab-0425a8c95266"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-30 05:48:13--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 108.159.227.65, 108.159.227.34, 108.159.227.26, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|108.159.227.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G  11.2MB/s    in 87s     \n",
            "\n",
            "2024-07-30 05:49:41 (13.7 MB/s) - ‘PhoBERT_base_fairseq.tar.gz’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "!tar -xzvf PhoBERT_base_transformers.tar.gz\n",
        "\n"
      ],
      "metadata": {
        "id": "PLrwpA-W1P3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f766ca5-66f0-48cd-c7ca-9d96b80da054"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-30 05:49:59--  https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 108.159.227.26, 108.159.227.65, 108.159.227.34, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|108.159.227.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322405979 (307M) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_transformers.tar.gz’\n",
            "\n",
            "PhoBERT_base_transf 100%[===================>] 307.47M  16.6MB/s    in 19s     \n",
            "\n",
            "2024-07-30 05:50:18 (16.6 MB/s) - ‘PhoBERT_base_transformers.tar.gz’ saved [322405979/322405979]\n",
            "\n",
            "PhoBERT_base_transformers/\n",
            "PhoBERT_base_transformers/config.json\n",
            "PhoBERT_base_transformers/bpe.codes\n",
            "PhoBERT_base_transformers/model.bin\n",
            "PhoBERT_base_transformers/dict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "import argparse"
      ],
      "metadata": {
        "id": "O0SCzInm1vsj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## following the instructions of phoBERT\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--bpe-codes',\n",
        "    default=\"/content/PhoBERT_base_transformers/bpe.codes\",\n",
        "    required=False,\n",
        "    type=str,\n",
        "    help='path to fastBPE BPE'\n",
        ")\n",
        "args, unknown = parser.parse_known_args()\n",
        "bpe = fastBPE(args)\n",
        "\n",
        "# Load the dictionary\n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/PhoBERT_base_transformers/dict.txt\")"
      ],
      "metadata": {
        "id": "QRrmVWLv7ftP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe.encode('Tôi là học sinh của trường Khoa học Tự Nhiên TPHCM. Tôi thích mèo.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N0wjrsib7r5r",
        "outputId": "b144aa1d-93f8-4f87-ac1c-f3e34f2d73c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tôi là học sinh của trường Khoa học Tự Nhiên TP@@ HCM. Tôi thích m@@ è@@ o.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.encode_line('<s> ' + 'Tôi là học sinh của trường Khoa học Tự Nhiên TP@@ HCM. Tôi thích m@@ è@@ o.' + ' </s>')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkk-Df4I90Kb",
        "outputId": "9f4af3c1-bcfd-4ede-8268-ecac26d4a656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,   218,     8,   222,   418,     7,   212,  3720,   222,  4544,\n",
              "        12567,  8656, 54251,   218,   543,  1387,  9367, 35211,     2,     2],\n",
              "       dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<s>` là token đặc biệt để đánh dấu vị trí đầu câu và `</s>` để đánh dấu vị trí cuối mỗi câu."
      ],
      "metadata": {
        "id": "D1CFajKx-Kyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "train_path = \"/content/dataSA/train.crash\"\n",
        "test_path = \"/content/dataSA/test.crash\""
      ],
      "metadata": {
        "id": "flSczBX4-N22"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_id, train_text, train_label = [], [], []\n",
        "test_id, test_text = [], []"
      ],
      "metadata": {
        "id": "hoLa46vLRFht"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_000000  \n",
        "\"Dung dc sp tot cam on  \n",
        "shop Đóng gói sản phẩm rất đẹp và chắc chắn Chất lượng sản phẩm tuyệt vời\"                \n",
        "0"
      ],
      "metadata": {
        "id": "z2OAfN6lR646"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_path, 'r') as f_r:\n",
        "    data = f_r.read().strip()\n",
        "    data = re.findall('train_[\\s\\S]+?\\\"\\n[01]\\n\\n', data)\n",
        "    # get the text between \" \"\n",
        "    for sample in data:\n",
        "        splits = sample.strip().split('\\n')\n",
        "\n",
        "        id = splits[0] #train_000000\n",
        "        label = int(splits[-1]) #0\n",
        "        text = ' '.join(splits[1:-1])[1:-1] # join each sentence with sapce and drop \" \"\n",
        "        text = rdrsegmenter.tokenize(text) # tokenize\n",
        "        text = ' '.join([' '.join(x) for x in text]) # join with space\n",
        "\n",
        "        train_id.append(id)\n",
        "        train_text.append(text)\n",
        "        train_label.append(label)"
      ],
      "metadata": {
        "id": "gvYKEEIfRPbg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(test_path, 'r') as f_r:\n",
        "    data = f_r.read().strip()\n",
        "    data = re.findall('test_[\\s\\S]+?\\\"\\n\\n', data)\n",
        "    for sample in data:\n",
        "        splits = sample.strip().split('\\n')\n",
        "\n",
        "        id = splits[0]\n",
        "        text = ' '.join(splits[1:])[1:-1]\n",
        "        text = rdrsegmenter.tokenize(text)\n",
        "        text = ' '.join([' '.join(x) for x in text])\n",
        "\n",
        "        test_id.append(id)\n",
        "        test_text.append(text)"
      ],
      "metadata": {
        "id": "jeY5pHLNTlL4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(len(train_id),len(test_id))\n",
        "print(np.unique(train_label, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFOVDSqoT4ye",
        "outputId": "031681e9-4ecf-4201-ef70-d2e4810b0b34"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16071 10980\n",
            "(array([0, 1]), array([8689, 7382]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sents, val_sents, train_labels, val_labels = train_test_split(train_text, train_label, test_size=0.1)\n"
      ],
      "metadata": {
        "id": "GSp4g8Ffa9kp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qh0kdh1pcV_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
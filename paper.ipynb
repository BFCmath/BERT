{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link paper: [here](https://arxiv.org/pdf/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is just a summary of what i can understand from the paper above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is designed to pretrain deep bidirectional representations from\n",
    "unlabeled text by jointly conditioning on both\n",
    "left and right context in all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 approaches for downstream task:\n",
    "+ feature based: uses task-specific architectures that\n",
    "include the pre-trained representations as additional features. \n",
    "+ fine tuned: introduces minimal\n",
    "task-specific parameters, and is trained on the\n",
    "downstream tasks by simply fine-tuning all pretrained parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Current limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major limitation is that standard language models are\n",
    "unidirectional, and this limits the choice of architectures that can be used during pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM (Masked language model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\n",
    "masked language model randomly masks some of\n",
    "the tokens from the input, and the objective is to\n",
    "predict the original vocabulary id of the masked word based only on its context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## NSP (Next Sentence Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "There are two steps\n",
    "+ pretrain: unlabeled data for different tasks\n",
    "+ fine tune: init with pretrain weights and fine tuned using labeled data for specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/bertmodel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above image, the pretrained step using MLM and NSP for bidirectional learning.  \n",
    "For specific tasks, the pretrained are then used to init. Note that each task has its own seperative models but they are all used the same pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Nearly the same as Transformer, just revise the old note\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Pretrain BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a deep bidirectional model is\n",
    "strictly more powerful than either a left-to-right\n",
    "model or the shallow concatenation of a left-toright and a right-to-left model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "MLM: e simply mask some percentage of the input\n",
    "tokens at random, and then predict those masked\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data generator\n",
    "chooses 15% of the token positions at random for\n",
    "prediction. If the i-th token is chosen, we replace\n",
    "the i-th token with (1) the [MASK] token 80% of\n",
    "the time (2) a random token 10% of the time (3)\n",
    "the unchanged i-th token 10% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSP:  In order\n",
    "to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically,\n",
    "when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual\n",
    "next sentence that follows A (labeled as IsNext),\n",
    "and 50% of the time it is a random sentence from\n",
    "the corpus (labeled as NotNext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Note: for the pretrain, this is unsupervised learning, with two specific task is predict the mising words and predict the next sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Bert\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common approach: bidirectional cross attention  \n",
    "read [here](https://arxiv.org/pdf/1611.01603)\n",
    "\n",
    "![bidirect](images/bidirect.png\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the old work will seperately encode the pair texts then use attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert approach: Concat 2 sentences and use self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
